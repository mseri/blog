---
title: "AI in Education - Some food for thought"
date: 2025-07-09T08:38:51+02:00
tags: [ai, education]
categories: ["Blog"]
---

What are we supposed to do about GenAI in (mathematical) education?
Many of us have been wrestling with it over the past few months. I certainly don't have a definitive answer, but it's a topic that demands we share what we're thinking and what we're seeing. If anything, it may bring a felt and needed opportunity to finally be able to rethink and reshape the way our education is organized. We discussed this in less uncertain times both in [It's Not Just Numbers](https://creators.spotify.com/pod/profile/not-just-numbers/episodes/S1E03---Teaching-mathematics--with-Tams-Grbe-and-Ceclia-Salgado-e2bsae3) and [various](https://creators.spotify.com/pod/profile/degrees-of-freedom/episodes/S2-Ep9---Specifications-Grading-e2oida7) [episodes](https://creators.spotify.com/pod/profile/degrees-of-freedom/episodes/S3E01-Critical-Pedagogy-and-the-Work-of-Paulo-Freire-e2oid9u) of [Degrees of Freedom](https://creators.spotify.com/pod/profile/degrees-of-freedom/episodes/S3E06---Ungrading-e2oidaq), our podcasts on mathematics and education respectively.

It helps to start with a realistic view of the technology. I like Andrej Karpathy's take on it: think of using an AI less like consulting an oracle and more like "asking the average data labeler on the internet." That framing cuts through a lot of the hype and re-grounds the conversation: these are statistical models trained by imitation, not logical reasoning machines.

As you probably know if you follow this rarely updated blog or if you follow me on social networks, I've been tinkering with these tools for quite a while. With time I settled to simple, tedious tasks where I can easily spot if something is wrong: generating ALT text for an image, getting a first draft of some Python code translated to JavaScript for a web demo, drawing pages for our kids, things like that... Itâ€™s a utility, a shortcut for boring work I could do myself and that I can easily check.

The real difficulty comes when we think about our classes and the use most of our stdents seem to be engaging with. Recently, I've started seeing homework submissions with what, citing a couple of colleagues, I like to call "froofs": things that look a lot like proofs, using the right words and symbols, but are fundamentally flawed or nonsensical. What worries me isn't really that the answer is wrong, but that the entire process of mathematical thinking has been bypassed. The struggle, the dead ends, the small breakthroughs, those are where the learning happens. A slick, confidently incorrect "froof" robs a student of that entire experience and, worse of all, of the learning itself.

By this I don't want to say, a priori, that these are bad and useless tools. As a user myself, I disagree with that. This is not to ignore the many ethical and environmental issues that come with the technology (what is worse is that they seem to have shadowed the similar discussion concerning cryptocurrencies instead of joining the club and increasing the societal pressure), but I think it is fool to believe that these tools are not here to stay, perhaps resized, once the hype fades, hopefully less ubiquitous, and way cheaper and less energy-hungry to run.

But let's close this side note and go back to the original point. I did have also positive experiences, with students using the tools to generate new exercises that they could work on, and check the correctness of. These have been good way for them to test their understanding and formulate clearer questions to ask to me in class or at tutorials. So there are, after all, useful ways to integrate these tools in learning. And you may already have played with them yourselves, and realized this.

I have been pondering a lot how could I rethink my courses to try and benefit from the situation. I'm not interested in an arms race of AI detection; it feels like a losing battle and a miserable way to interact with students. This semester, I'm trying an experiment: I've made homework ungraded. The idea is to lower the stakes and, hopefully, remove the incentive to get the perfect answer, allowing more scope for exploration and feedback. The goal is for homework to be a place for genuine practice. We then use class and tutorial time for peer-feedback, which I hope encourages them to engage with the material and each other's reasoning more critically. The actual assessments of understanding are moved fully to the midterm and final exams.

I don't know if this is the "right" answer. It's just one attempt to adapt to a new reality. It feels more productive than simply banning the tools or pretending they don't exist. But it's an open question, and I suspect there are many different approaches being tried out there.

What are others trying? How are these tools showing up in your classrooms? I'm genuinely curious to hear other experiences and ideas. In the meantime I have collected my thoughts, experiences and experiments, as well
as some literature and nice tools, in a set of slides that we used to start the discussion at our department. I share them below since a few colleagues have been asking for them. Please, feel free to contact me to share your experiences!

{{< pdfreader url="attachments/25.06.08-AI_in_class.pdf" height="400px" >}}
